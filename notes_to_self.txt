
> On the subject of shenanigans
"Motivating your decisions is key here, 
if you can motivate it, then it is fine. 
Remember that motivation needs to be well thought out, 
as you need to state that your solution has benefits over the obivious ones (like sets)."
        - Word of god, Tobias Andersson Gidlund.

For the project, make it very clear why certain tradeoffs were made like the exclusion of sets.

> On the subject of flow
currently the project is a mess, we need to create a very tangible distinction between functions/methods that:
    1. return readable human text
    2. print readable human text
    3. return json data
    4. return dicts
    5. return tuples

we need to figure out - *what* needs to be exported, *what* needs to be shown to the user, and how can we have the underlying data be the same?
ideally i'd want the following:

hy_analysis.py populates hy_tracked_textfiles HyTextFile objects with statistics (dicts, tuples, etc)
hy_tracked_textfiles can be queried with, for instance, .get_top_words() and get python types back. this is fine.
hy_fetch_json will query hy_tracked_textfiles to get data which is serialized into JSON. This serialized JSON terminates the chain - this is what will be exported.
alternatively, hy_prettify can query hy_tracked_textfiles, also gets data, but this is turned into a human-friendly format, and returned. Main.py calls hy_prettify and prints the results.

NO VIOLATIONS... okay kinda whatever.

> On the subject of "Something that will amaze us"
How about trigram analysis for language identification? 
Maintain a store of known trigrams for english, swedish, german, and hungarian, then find each trigram in the selected file and find
whichever store it matches closest. Extensible for more languages, kind of cool. This alongside the ability to compare statistics of several loaded
files should hopefully qualify for the program extensions for A grade.

This would also be pretty simple to implement.
Make a function which calculates the distribution of trigrams in a text. For optimization purposes, perhaps only word boundary trigrams.
Run a few texts in various languages through this to get a general distribution of trigrams (2.5% THE in English, for instance).
Store these samples under language_samples/english.json.

When analyzing a new text, compute the trigram distribution using the same algorithm and map to the closest known sample. ezpz
Sum of absolute differences or cosine similarity. Ideally cosine similarity.
cosine = dot_product(A, B) / (norm(A) * norm(B))
where dot_product(A, B) is the sum of A[t] * B[t] for all trigrams t.
norm(A) is the square root of the sum of A[t]^2

remember to only take the first, like, 2000 lines (or maybe 100000 characters? if it's all a single line it'll still mess up) since trigram analysis converges and we dont wanna burn CPU cycles (...i mean, don't we? we're using fucking python.) 
